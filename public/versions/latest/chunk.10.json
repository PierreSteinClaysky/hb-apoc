{"Devices Ecosystem/03_dev_scheduler_formats_analysis.md":{"content":"# `dev_scheduler_formats.erl` Analysis\r\n\r\n## Overview\r\n\r\n`dev_scheduler_formats.erl` is a specialized module that provides format conversion capabilities for the scheduler subsystem in HyperBEAM. It serves as a compatibility layer, enabling communication between HyperBEAM's internal representation of assignments and various external client formats, particularly focusing on supporting legacy AO clients.\r\n\r\nAs noted in the module's documentation, it provides support for two main formats:\r\n- `application/json` - described as a legacy format not recommended for new integrations\r\n- `application/http` - the preferred format for newer integrations\r\n\r\nThe module implements bidirectional conversion between these formats, allowing both the generation of client-compatible representations from internal data structures and the parsing of client-provided data into internal formats. This facilitates interoperability across different versions and implementations of the AO protocol.\r\n\r\n## Key Characteristics\r\n\r\n- **Format Conversion**: Provides functions to convert between internal and external representations\r\n- **Legacy Support**: Maintains compatibility with older AO protocol formats\r\n- **Bidirectional Transformation**: Supports both encoding to and decoding from different formats\r\n- **Normalization Logic**: Includes specialized handling for type and field name normalization\r\n- **JSON Integration**: Handles encoding and decoding of JSON structures\r\n- **Cursor Generation**: Creates cursor values for paginated responses\r\n- **Field Mapping**: Maps between different field naming conventions\r\n\r\n## Dependencies\r\n\r\n### Upstream Dependencies\r\n\r\n- `hb_converge`: For accessing message fields\r\n- `hb_util`: For utility functions like encoding/decoding and type conversion\r\n- `hb_gateway_client`: For message conversion functionality\r\n- `dev_json_iface`: For converting messages to JSON structures\r\n- `ar_timestamp`: For obtaining blockchain timestamp information\r\n- `jiffy`: For JSON encoding\r\n\r\n## Implementation Details\r\n\r\n### Conversion to HTTP Format\r\n\r\nThe `assignments_to_bundle/4` function converts a list of assignments to the HTTP bundle format:\r\n\r\n```erlang\r\nassignments_to_bundle(ProcID, Assignments, More, Opts) ->\r\n    TimeInfo = ar_timestamp:get(),\r\n    assignments_to_bundle(ProcID, Assignments, More, TimeInfo, Opts).\r\nassignments_to_bundle(ProcID, Assignments, More, TimeInfo, Opts) ->\r\n    {Timestamp, Height, Hash} = TimeInfo,\r\n    {ok, #{\r\n        <<\"type\">> => <<\"schedule\">>,\r\n        <<\"process\">> => hb_util:human_id(ProcID),\r\n        <<\"continues\">> => atom_to_binary(More, utf8),\r\n        <<\"timestamp\">> => hb_util:int(Timestamp),\r\n        <<\"block-height\">> => hb_util:int(Height),\r\n        <<\"block-hash\">> => hb_util:human_id(Hash),\r\n        <<\"assignments\">> =>\r\n            maps:from_list(\r\n                lists:map(\r\n                    fun(Assignment) ->\r\n                        {\r\n                            hb_converge:get(\r\n                                <<\"slot\">>,\r\n                                Assignment,\r\n                                Opts#{ hashpath => ignore }\r\n                            ),\r\n                            Assignment\r\n                        }\r\n                    end,\r\n                    Assignments\r\n                )\r\n            )\r\n    }}.\r\n```\r\n\r\nThis function creates a structured representation that includes metadata about the process and its context (like timestamp and block information), along with the assignments themselves.\r\n\r\n### Conversion to AOS2 Format\r\n\r\nThe `assignments_to_aos2/4` function converts assignments to the legacy AOS2 JSON format:\r\n\r\n```erlang\r\nassignments_to_aos2(ProcID, Assignments, More, Opts) when is_map(Assignments) ->\r\n    SortedKeys =\r\n        lists:sort(\r\n            lists:map(\r\n                fun hb_util:int/1,\r\n                maps:keys(\r\n                    maps:without(\r\n                        [<<\"priv\">>, <<\"attestations\">>],\r\n                        Assignments\r\n                    )\r\n                )\r\n            )\r\n        ),\r\n    ListAssignments =\r\n        lists:map(\r\n            fun(Key) ->\r\n                hb_converge:get(Key, Assignments, Opts)\r\n            end,\r\n            SortedKeys\r\n        ),\r\n    assignments_to_aos2(ProcID, ListAssignments, More, Opts);\r\n```\r\n\r\nThis function sorts assignments by their slot numbers and then formats them according to the AOS2 specification, which includes a different structure with \"edges\" and \"nodes\" along with pagination information.\r\n\r\n### Cursor Generation\r\n\r\nThe module provides cursor generation for paginated responses:\r\n\r\n```erlang\r\ncursor(Assignment, Opts) ->\r\n    hb_converge:get(<<\"slot\">>, Assignment, Opts#{ hashpath => ignore }).\r\n```\r\n\r\nIn this implementation, the cursor is simply the slot number of the assignment, which allows clients to request the next page of assignments starting from a specific slot.\r\n\r\n### Assignment Conversion\r\n\r\nThe `assignment_to_aos2/2` function converts an individual assignment to AOS2 format:\r\n\r\n```erlang\r\nassignment_to_aos2(Assignment, Opts) ->\r\n    Message = hb_converge:get(<<\"body\">>, Assignment, Opts),\r\n    AssignmentWithoutBody = maps:without([<<\"body\">>], Assignment),\r\n    {[\r\n        {<<\"message\">>,\r\n            dev_json_iface:message_to_json_struct(Message)},\r\n        {<<\"assignment\">>,\r\n            dev_json_iface:message_to_json_struct(AssignmentWithoutBody)}\r\n    ]}.\r\n```\r\n\r\nThis function separates the assignment's body (the actual message) from its metadata, and converts both parts to JSON structures using the `dev_json_iface` module.\r\n\r\n### Conversion from AOS2\r\n\r\nThe module also supports converting from AOS2 format back to internal format:\r\n\r\n```erlang\r\naos2_to_assignments(ProcID, Body, Opts) ->\r\n    Assignments = maps:get(<<\"edges\">>, Body, Opts),\r\n    ?event({raw_assignments, Assignments}),\r\n    ParsedAssignments =\r\n        lists:map(\r\n            fun(A) -> aos2_to_assignment(A, Opts) end,\r\n            Assignments\r\n        ),\r\n    ?event({parsed_assignments, ParsedAssignments}),\r\n    TimeInfo =\r\n        case ParsedAssignments of\r\n            [] -> {0, 0, hb_util:encode(<<0:256>>)};\r\n            _ ->\r\n                Last = lists:last(ParsedAssignments),\r\n                {\r\n                    hb_converge:get(<<\"timestamp\">>, Last, Opts),\r\n                    hb_converge:get(<<\"block-height\">>, Last, Opts),\r\n                    hb_converge:get(<<\"block-hash\">>, Last, Opts)\r\n                }\r\n        end,\r\n    assignments_to_bundle(ProcID, ParsedAssignments, false, TimeInfo, Opts).\r\n```\r\n\r\nThis function extracts assignments from an AOS2 response, converts each one to the internal format, and then packages them into a bundle response.\r\n\r\n### Type Normalization\r\n\r\nThe module includes sophisticated type normalization to handle differences in data representation between formats:\r\n\r\n```erlang\r\naos2_normalize_types(Msg = #{ <<\"timestamp\">> := TS }) when is_binary(TS) ->\r\n    aos2_normalize_types(Msg#{ <<\"timestamp\">> => hb_util:int(TS) });\r\naos2_normalize_types(Msg = #{ <<\"nonce\">> := Nonce })\r\n        when is_binary(Nonce) and not is_map_key(<<\"slot\">>, Msg) ->\r\n    aos2_normalize_types(\r\n        Msg#{ <<\"slot\">> => hb_util:int(Nonce) }\r\n    );\r\n% ... additional normalization rules ...\r\n```\r\n\r\nThis function handles various format-specific quirks, such as:\r\n- Converting binary timestamps to integers\r\n- Mapping `nonce` fields to `slot` fields\r\n- Ensuring consistent data types across fields\r\n- Providing default values for missing fields\r\n\r\n## Questions and Insights\r\n\r\n### Questions\r\n\r\n1. **Verifiability Impact**: The code comments note that the conversion to AOS2 format is \"destructive to the verifiability of the assignment.\" What specific aspects of verifiability are lost, and are there plans to address this in future versions?\r\n\r\n2. **Format Evolution**: Given that the JSON format is described as \"legacy,\" what is the roadmap for format evolution? Will new formats be added in the future?\r\n\r\n3. **Performance Considerations**: How does the format conversion impact performance, especially for large numbers of assignments?\r\n\r\n4. **Error Handling**: How are malformed or incompatible formats handled? Are there validation mechanisms beyond what's visible in this module?\r\n\r\n5. **Version Management**: How are format version changes managed over time? Are there compatibility checks to ensure that different versions can interoperate?\r\n\r\n### Insights\r\n\r\n1. **Compatibility Layer**: The module serves as an essential compatibility layer, enabling HyperBEAM to work with various client implementations despite differences in data representation.\r\n\r\n2. **Format Preference**: The documentation explicitly indicates that `application/json` is a legacy format, suggesting a clear direction for future development and integrations.\r\n\r\n3. **Field Mapping Intelligence**: The normalization logic includes intelligent field mapping (like `nonce` to `slot`) that demonstrates an understanding of the semantic relationships between different field names.\r\n\r\n4. **Bidirectional Capability**: The ability to both encode to and decode from different formats provides flexibility in how HyperBEAM interacts with external systems.\r\n\r\n5. **Gradual Migration Strategy**: The maintenance of legacy format support while indicating a preferred format suggests a gradual migration strategy for ecosystem participants.\r\n\r\n## Integration with Other Subsystems\r\n\r\n### Integration with Device and Process Management Subsystem\r\n\r\n- Directly supports `dev_scheduler.erl` by providing format conversion capabilities\r\n- Enables interoperability between HyperBEAM's internal assignment representation and client-facing formats\r\n- Facilitates the retrieval and transmission of scheduler assignments in appropriate formats\r\n\r\n### Integration with Codec and Data Format Subsystem\r\n\r\n- Uses `dev_json_iface` for message-to-JSON conversion\r\n- Works with the `jiffy` library for JSON encoding\r\n- Implements conversion logic that bridges between different data representation schemes\r\n\r\n### Integration with Arweave Subsystem\r\n\r\n- Uses `ar_timestamp` to obtain blockchain timestamp information\r\n- Includes blockchain-specific metadata in formatted responses\r\n- Handles Arweave-specific field naming and typing conventions\r\n\r\n### Integration with Core Infrastructure\r\n\r\n- Leverages `hb_converge` for message field access\r\n- Uses `hb_util` for various utility functions\r\n- Works with `hb_gateway_client` for certain message conversion operations\r\n\r\n## Recategorization Considerations\r\n\r\nThis module is correctly categorized as part of the Device and Process Management Subsystem, as its primary purpose is to support the scheduler functionality by enabling communication between the scheduler and various clients.\r\n\r\nWhile it has aspects that might associate it with the Codec and Data Format Subsystem (given its focus on format conversion), its functionality is specifically tailored to the needs of the scheduler, making it an integral part of the scheduler subsystem rather than a general-purpose codec component.\r\n\r\nThe module's tight integration with `dev_scheduler.erl` and its focus on scheduler-specific concerns (like assignments and slots) further reinforces its proper categorization. It represents a specialized auxiliary component that enhances the scheduler's ability to interact with the broader ecosystem through appropriate format adaptations.\r\n"},"Devices Ecosystem/04_dev_scheduler_registry_analysis.md":{"content":"# `dev_scheduler_registry.erl` Analysis\r\n\r\n## Overview\r\n\r\n`dev_scheduler_registry.erl` is a registry module for the HyperBEAM system that manages the lifecycle and discovery of scheduler processes. As described in its documentation, it serves as \"a simple registry for local services in AO, using pg,\" with a current focus on scheduler processes (referred to as \"SU processes\" in the code comments).\r\n\r\nThis module provides a centralized mechanism for mapping between process IDs (which are typically content-addressed identifiers) and the actual Erlang processes that handle these identifiers. It also offers process creation capabilities when requested processes don't exist, making it both a registry and a factory for scheduler processes.\r\n\r\nThe registry uses the `hb_name` service, which provides a distributed naming capability, enabling processes to be located across the system. This is essential for the distributed nature of HyperBEAM operations.\r\n\r\n## Key Characteristics\r\n\r\n- **Process Registration**: Maps between process IDs and Erlang PIDs\r\n- **Process Discovery**: Enables lookup of running scheduler processes\r\n- **Process Creation**: Can optionally create processes that don't exist\r\n- **Wallet Management**: Provides access to the wallet used for scheduler operations\r\n- **Process Enumeration**: Supports listing all registered processes\r\n- **Naming Service Integration**: Uses `hb_name` for distributed process registration\r\n\r\n## Dependencies\r\n\r\n### Upstream Dependencies\r\n\r\n- `hb_name`: For process registration and lookup\r\n- `hb`: For wallet access\r\n- `dev_scheduler_server`: For starting new scheduler processes\r\n\r\n## Implementation Details\r\n\r\n### Registry Initialization\r\n\r\nThe module initializes the registry system by starting the naming service:\r\n\r\n```erlang\r\nstart() ->\r\n    hb_name:start(),\r\n    ok.\r\n```\r\n\r\nThis ensures that the underlying name service is available for process registration and lookup.\r\n\r\n### Process Lookup\r\n\r\nThe module provides a set of overloaded `find` functions for looking up processes:\r\n\r\n```erlang\r\nfind(ProcID) -> find(ProcID, false).\r\nfind(ProcID, GenIfNotHosted) ->\r\n    find(ProcID, GenIfNotHosted, #{ priv_wallet => hb:wallet() }).\r\nfind(ProcID, GenIfNotHosted, Opts) ->\r\n    case hb_name:lookup({dev_scheduler, ProcID}) of\r\n        undefined -> maybe_new_proc(ProcID, GenIfNotHosted, Opts);\r\n        Pid -> Pid\r\n    end.\r\n```\r\n\r\nThese functions attempt to find a process by its ID, with the option to create a new process if one doesn't exist. The `GenIfNotHosted` parameter controls whether a new process should be created when not found:\r\n- When `GenIfNotHosted` is `false`, the function returns `not_found` for non-existent processes\r\n- When `GenIfNotHosted` is `true`, it creates a new process via `dev_scheduler_server:start/2`\r\n\r\nThe lookup is performed using the `hb_name:lookup/1` function, with process IDs prefixed with `dev_scheduler` to create a namespaced identifier.\r\n\r\n### Process Creation\r\n\r\nThe `maybe_new_proc/3` function handles the conditional creation of new processes:\r\n\r\n```erlang\r\nmaybe_new_proc(_ProcID, false, _Opts) -> not_found;\r\nmaybe_new_proc(ProcID, _GenIfNotHosted, Opts) -> \r\n    dev_scheduler_server:start(ProcID, Opts).\r\n```\r\n\r\nThis function either returns `not_found` or delegates to `dev_scheduler_server:start/2` to create a new scheduler process for the given ID.\r\n\r\n### Process Enumeration\r\n\r\nThe `get_processes/0` function retrieves a list of all registered process IDs:\r\n\r\n```erlang\r\nget_processes() ->\r\n    ?event({getting_processes, hb_name:all()}),\r\n    [ ProcID || {{dev_scheduler, ProcID}, _} <- hb_name:all() ].\r\n```\r\n\r\nThis function filters the complete set of registered names from `hb_name:all()` to extract only those that are prefixed with `dev_scheduler`, returning just the process IDs.\r\n\r\n### Wallet Access\r\n\r\nThe `get_wallet/0` function provides access to the wallet used for authentication:\r\n\r\n```erlang\r\nget_wallet() ->\r\n    % TODO: We might want to use a different wallet per SU later.\r\n    hb:wallet().\r\n```\r\n\r\nAs noted in the code comment, there's a consideration for potentially using different wallets per scheduler unit in the future.\r\n\r\n## Questions and Insights\r\n\r\n### Questions\r\n\r\n1. **Scalability Considerations**: How does the registry handle scenarios with a large number of processes? Are there any performance bottlenecks in the lookup or enumeration operations?\r\n\r\n2. **Wallet Isolation**: The comment about potentially using different wallets per scheduler unit suggests a security or isolation consideration. What are the implications of using a single wallet versus multiple wallets?\r\n\r\n3. **Process Lifecycle Management**: How are process terminations handled in the registry? Is there a mechanism to clean up entries for processes that have ended?\r\n\r\n4. **Distributed Registry**: How does the registry behave in a distributed environment with multiple nodes? How is consistency maintained across nodes?\r\n\r\n5. **Lookup Performance**: Are there any optimizations for frequent lookups of the same process ID? Is there any caching mechanism?\r\n\r\n### Insights\r\n\r\n1. **Centralized Discovery**: The registry provides a centralized point of discovery for scheduler processes, simplifying the interaction model for other components that need to work with these processes.\r\n\r\n2. **Factory Pattern**: The module implements a factory pattern through its conditional process creation capability, combining discovery and creation in a convenient interface.\r\n\r\n3. **Simple Interface**: The API is designed to be straightforward, with sensible defaults and overloaded functions to accommodate different use cases.\r\n\r\n4. **Naming Convention**: The use of a consistent prefix (`dev_scheduler`) for registered processes creates a namespace that allows for easy filtering and organization.\r\n\r\n5. **Testing Focus**: The comprehensive test suite indicates a focus on reliability, covering different scenarios including non-existent processes, process creation, and enumeration.\r\n\r\n## Integration with Other Subsystems\r\n\r\n### Integration with Device and Process Management Subsystem\r\n\r\n- Serves as a critical component for the scheduler subsystem, enabling process discovery and creation\r\n- Works closely with `dev_scheduler_server` to instantiate new scheduler processes\r\n- Provides the wallet access needed for scheduler operations\r\n\r\n### Integration with Core Infrastructure\r\n\r\n- Uses `hb_name` for process registration and lookup\r\n- Relies on `hb` for wallet access\r\n\r\n## Recategorization Considerations\r\n\r\nThis module is correctly categorized as part of the Device and Process Management Subsystem. Its primary responsibility is managing the lifecycle and discovery of scheduler processes, which is a fundamental aspect of process management.\r\n\r\nThe module's tight integration with `dev_scheduler_server` and its focus on process mapping and creation align it firmly with the process management domain. While it depends on core infrastructure components like `hb_name`, its purpose is specifically to support the process management aspects of the system.\r\n\r\nThe registry pattern implemented by this module is a common design pattern in distributed systems, particularly for service discovery and lifecycle management. This further reinforces its categorization as a process management component rather than an infrastructure or utility component.\r\n"},"Devices Ecosystem/05_dev_scheduler_server_analysis.md":{"content":"# `dev_scheduler_server.erl` Analysis\r\n\r\n## Overview\r\n\r\n`dev_scheduler_server.erl` implements a server component for the HyperBEAM scheduler system that manages the assignment of messages to specific slots within a process. As noted in its documentation, it \"acts as a deliberate 'bottleneck' to prevent the server accidentally assigning multiple messages to the same slot,\" highlighting its role in maintaining ordering guarantees within the system.\r\n\r\nThis module is designed as a long-lived Erlang process that maintains state for a specific process ID, including the current slot number and a cryptographic hash chain that links all assignments together. When a message is scheduled, the server assigns it to the next available slot, updates the hash chain, and creates an assignment message that is persisted to storage and potentially uploaded to the network.\r\n\r\nThe server supports different scheduling modes that provide different trade-offs between performance and confirmation guarantees, ranging from aggressive asynchronous scheduling to fully synchronized operations that wait for network confirmation.\r\n\r\n## Key Characteristics\r\n\r\n- **Sequential Assignment**: Ensures messages are assigned to sequential slots without gaps or duplicates\r\n- **Hash Chain Management**: Maintains a cryptographic chain linking all assignments together\r\n- **Multiple Scheduling Modes**: Supports different performance/reliability trade-offs through configurable modes\r\n- **Process-Per-ID Model**: Creates a dedicated Erlang process for each HyperBEAM process ID\r\n- **State Management**: Maintains and persists state between restarts\r\n- **Erlang Message Passing**: Uses Erlang's message passing for communication\r\n- **Blockchain Integration**: Includes Arweave blockchain metadata in assignments\r\n\r\n## Dependencies\r\n\r\n### Upstream Dependencies\r\n\r\n- `hb_name`: For process registration\r\n- `dev_scheduler_cache`: For persisting and retrieving assignments\r\n- `hb_message`: For creating attested assignment messages\r\n- `hb_client`: For uploading assignments to the network\r\n- `hb_path`: For path extraction from messages\r\n- `hb_opts`: For configuration options\r\n- `ar_timestamp`: For obtaining blockchain timing information\r\n- `hb`: For wallet access\r\n\r\n## Implementation Details\r\n\r\n### Server Initialization\r\n\r\nThe `start/2` function initializes a new scheduler server process:\r\n\r\n```erlang\r\nstart(ProcID, Opts) ->\r\n    ?event(scheduling, {starting_scheduling_server, {proc_id, ProcID}}),\r\n    spawn_link(\r\n        fun() ->\r\n            case hb_opts:get(scheduling_mode, disabled, Opts) of\r\n                disabled ->\r\n                    throw({scheduling_disabled_on_node, {requested_for, ProcID}});\r\n                _ -> ok\r\n            end,\r\n            hb_name:register({dev_scheduler, ProcID}),\r\n            {CurrentSlot, HashChain} =\r\n                case dev_scheduler_cache:latest(ProcID, Opts) of\r\n                    not_found ->\r\n                        ?event({starting_new_schedule, {proc_id, ProcID}}),\r\n                        {-1, <<>>};\r\n                    {Slot, Chain} ->\r\n                        ?event({continuing_schedule, {proc_id, ProcID}, {current_slot, Slot}}),\r\n                        {Slot, Chain}\r\n                end,\r\n            ?event(\r\n                {scheduler_got_process_info,\r\n                    {proc_id, ProcID},\r\n                    {current, CurrentSlot},\r\n                    {hash_chain, HashChain}\r\n                }\r\n            ),\r\n            server(\r\n                #{\r\n                    id => ProcID,\r\n                    current => CurrentSlot,\r\n                    wallet => hb_opts:get(priv_wallet, hb:wallet(), Opts),\r\n                    hash_chain => HashChain,\r\n                    opts => Opts\r\n                }\r\n            )\r\n        end\r\n    ).\r\n```\r\n\r\nThis function:\r\n1. Checks if scheduling is enabled for the node\r\n2. Registers the process with the naming service\r\n3. Retrieves the current state (slot and hash chain) from the cache if available, or initializes new state if not\r\n4. Starts the server loop with the initial state\r\n\r\n### Message Scheduling\r\n\r\nThe `schedule/2` function is the main interface for scheduling messages:\r\n\r\n```erlang\r\nschedule(AOProcID, Message) when is_binary(AOProcID) ->\r\n    schedule(dev_scheduler_registry:find(AOProcID), Message);\r\nschedule(ErlangProcID, Message) ->\r\n    ErlangProcID ! {schedule, Message, self()},\r\n    receive\r\n        {scheduled, Message, Assignment} ->\r\n            Assignment\r\n    end.\r\n```\r\n\r\nThis function:\r\n1. Resolves the process ID to an Erlang PID if necessary\r\n2. Sends a scheduling request to the scheduler process\r\n3. Waits for a confirmation response with the assignment\r\n\r\n### Assignment Creation\r\n\r\nThe core scheduling logic is in the `do_assign/3` function:\r\n\r\n```erlang\r\ndo_assign(State, Message, ReplyPID) ->\r\n    HashChain = next_hashchain(maps:get(hash_chain, State), Message),\r\n    NextSlot = maps:get(current, State) + 1,\r\n    % Run the signing of the assignment and writes to the disk in a separate\r\n    % process.\r\n    AssignFun =\r\n        fun() ->\r\n            {Timestamp, Height, Hash} = ar_timestamp:get(),\r\n            Assignment = hb_message:attest(#{\r\n                <<\"path\">> =>\r\n                    case hb_path:from_message(request, Message) of\r\n                        undefined -> <<\"compute\">>;\r\n                        Path -> Path\r\n                    end,\r\n                <<\"data-protocol\">> => <<\"ao\">>,\r\n                <<\"variant\">> => <<\"ao.N.1\">>,\r\n                <<\"process\">> => hb_util:id(maps:get(id, State)),\r\n                <<\"epoch\">> => <<\"0\">>,\r\n                <<\"slot\">> => NextSlot,\r\n                <<\"block-height\">> => Height,\r\n                <<\"block-hash\">> => hb_util:human_id(Hash),\r\n                <<\"block-timestamp\">> => Timestamp,\r\n                % Note: Local time on the SU, not Arweave\r\n                <<\"timestamp\">> => erlang:system_time(millisecond),\r\n                <<\"hash-chain\">> => hb_util:id(HashChain),\r\n                <<\"body\">> => Message\r\n            }, maps:get(wallet, State)),\r\n            % ... storage and reply logic ...\r\n        end,\r\n    % ... scheduling mode handling ...\r\n    State#{\r\n        current := NextSlot,\r\n        hash_chain := HashChain\r\n    }.\r\n```\r\n\r\nThis function:\r\n1. Creates the next hash chain link\r\n2. Determines the next slot number\r\n3. Creates an assignment message with blockchain metadata, process information, and the message itself\r\n4. Handles storage, network upload, and client notification based on the scheduling mode\r\n5. Updates and returns the server state\r\n\r\n### Hash Chain Management\r\n\r\nThe `next_hashchain/2` function maintains the cryptographic chain of assignments:\r\n\r\n```erlang\r\nnext_hashchain(HashChain, Message) ->\r\n    ?event({creating_next_hashchain, {hash_chain, HashChain}, {message, Message}}),\r\n    ID = hb_message:id(Message, all),\r\n    crypto:hash(\r\n        sha256,\r\n        << HashChain/binary, ID/binary >>\r\n    ).\r\n```\r\n\r\nThis function:\r\n1. Extracts the content-addressed ID of the message\r\n2. Concatenates it with the previous hash chain\r\n3. Computes a new SHA-256 hash of the combined data\r\n\r\n### Scheduling Modes\r\n\r\nThe module supports different scheduling modes, implemented in the `maybe_inform_recipient/5` function:\r\n\r\n```erlang\r\nmaybe_inform_recipient(Mode, ReplyPID, Message, Assignment, State) ->\r\n    case hb_opts:get(scheduling_mode, remote_confirmation, maps:get(opts, State)) of\r\n        Mode -> ReplyPID ! {scheduled, Message, Assignment};\r\n        _ -> ok\r\n    end.\r\n```\r\n\r\nThis function:\r\n1. Checks if the current scheduling mode matches the requested notification mode\r\n2. Sends a confirmation message to the client if the modes match\r\n\r\nThe supported modes are:\r\n- `aggressive`: Responds immediately and performs the assignment in a separate process\r\n- `local_confirmation`: Responds after writing to local storage\r\n- `remote_confirmation`: Responds after uploading to the network\r\n\r\n## Questions and Insights\r\n\r\n### Questions\r\n\r\n1. **Concurrency Control**: How does the system handle concurrent scheduling requests for the same process? Is there a mechanism to prevent race conditions between multiple instances of scheduling servers?\r\n\r\n2. **Failure Recovery**: What happens if the scheduling server crashes during an assignment operation? How is consistency maintained in such scenarios?\r\n\r\n3. **Network Partition Handling**: How does the system handle network partitions, particularly in `remote_confirmation` mode where uploads are expected to succeed?\r\n\r\n4. **Performance Implications**: What are the performance implications of the different scheduling modes? Are there benchmarks or guidelines for choosing between them?\r\n\r\n5. **Backward Compatibility**: How does the server handle backward compatibility with older message formats or hash chain algorithms?\r\n\r\n### Insights\r\n\r\n1. **Deliberate Bottleneck**: The server is explicitly designed as a bottleneck, which is an interesting architectural choice. This indicates a deliberate trade-off between parallelism and sequential consistency.\r\n\r\n2. **Cryptographic Continuity**: The hash chain mechanism ensures that each assignment is cryptographically linked to its predecessors, creating a verifiable history of assignments.\r\n\r\n3. **Flexible Confirmation Models**: The three scheduling modes provide a spectrum of confirmation guarantees, allowing applications to choose the right balance between performance and reliability.\r\n\r\n4. **State Persistence**: The server is designed to recover its state from persistent storage, allowing it to continue from the correct slot after restarts.\r\n\r\n5. **Blockchain Integration**: The inclusion of Arweave blockchain metadata in assignments ties the scheduler to the blockchain timeline, potentially enabling external verification.\r\n\r\n## Integration with Other Subsystems\r\n\r\n### Integration with Device and Process Management Subsystem\r\n\r\n- Works with `dev_scheduler_registry` for process discovery and creation\r\n- Provides the core scheduling logic for `dev_scheduler`\r\n- Maintains the state that other scheduler components refer to\r\n\r\n### Integration with Storage Subsystem\r\n\r\n- Uses `dev_scheduler_cache` for persisting and retrieving assignments\r\n- Creates a permanent record of assignments for future reference\r\n\r\n### Integration with Network Communication Subsystem\r\n\r\n- Uses `hb_client` to upload assignments to the network\r\n- Potentially waits for network confirmation in `remote_confirmation` mode\r\n\r\n### Integration with Core Infrastructure\r\n\r\n- Uses `hb_message` for message attestation\r\n- Uses `hb_name` for process registration\r\n- Uses `hb_opts` for configuration access\r\n\r\n### Integration with Arweave Subsystem\r\n\r\n- Uses `ar_timestamp` to obtain blockchain timing information\r\n- Incorporates blockchain metadata into assignments\r\n\r\n## Recategorization Considerations\r\n\r\nThis module is correctly categorized as part of the Device and Process Management Subsystem. Its primary responsibility is managing the scheduling of messages within processes, which is a core aspect of process management.\r\n\r\nThe module's tight integration with other scheduler components (`dev_scheduler_registry`, `dev_scheduler_cache`) and its focused responsibility of maintaining the sequential order of message execution further reinforce its categorization.\r\n\r\nWhile it interacts with other subsystems such as storage and network, these interactions are in service of its primary process management responsibility. The deliberate design as a bottleneck for sequential processing also aligns with the process management paradigm rather than with the patterns typical of other subsystems.\r\n"},"Devices Ecosystem/06_scheduler_subsystem_overview.md":{"content":"# Device and Process Management Subsystem Overview\r\n\r\n## Introduction\r\n\r\nThe Device and Process Management Subsystem is a cornerstone of the HyperBEAM architecture, providing the extensible computation framework that powers the system's capabilities. This subsystem encompasses both the device infrastructure—which provides a pluggable model for computation units—and the process management system that coordinates the execution of these devices in a secure, verifiable manner.\r\n\r\nAt its core, this subsystem implements a novel approach to extensible computing: the device-based architecture. Devices in HyperBEAM are pluggable computation units that can be combined, composed, and orchestrated to create complex processing pipelines. This modular approach enables the system to be highly extensible, adding new functionality without modifying core code.\r\n\r\nHaving analyzed ten key components of this subsystem, we can now present a holistic view of how they work together to provide a robust framework for distributed, verifiable computation.\r\n\r\n## Component Overview\r\n\r\nThe Device and Process Management Subsystem encompasses three major functional areas with ten primary components:\r\n\r\n### Device Infrastructure\r\n\r\n1. **`dev_message.erl`**: The identity device serving as the foundation for message field access, manipulation, and attestation handling. It acts as the base device that all other devices build upon.\r\n\r\n2. **`dev_stack.erl`**: A meta-device that manages the execution of device stacks, enabling complex device composition and orchestration through fold (sequential processing) and map (parallel execution) modes.\r\n\r\n### Process Management\r\n\r\n3. **`dev_process.erl`**: The core process orchestration device that implements the process state machine and routes operations to specialized devices through a device-swapping pattern.\r\n\r\n4. **`dev_process_cache.erl`**: A specialized caching module for process computation results, providing dual-indexing (slot and message ID) for efficient state retrieval.\r\n\r\n5. **`dev_process_worker.erl`**: A long-lived worker process system that maintains in-memory state between computation steps for performance optimization.\r\n\r\n### Scheduler System\r\n\r\n6. **`dev_scheduler.erl`**: The device implementation that provides the public interface for interacting with the scheduler, dispatching operations to the appropriate components.\r\n\r\n7. **`dev_scheduler_server.erl`**: A long-lived Erlang process that manages assignments for a specific process ID, maintaining state and ensuring sequential slot assignment.\r\n\r\n8. **`dev_scheduler_cache.erl`**: Provides storage and retrieval functionality for scheduler assignments, enabling persistence and recovery of scheduler state.\r\n\r\n9. **`dev_scheduler_registry.erl`**: Manages the lifecycle and discovery of scheduler processes, acting as both a registry and factory for scheduler server instances.\r\n\r\n10. **`dev_scheduler_formats.erl`**: Handles the conversion between internal assignment representation and various client-facing formats, providing backward compatibility and format transformation.\r\n\r\n## Architectural Design\r\n\r\n### Design Patterns\r\n\r\nThe Device and Process Management Subsystem employs several significant design patterns that span across its components:\r\n\r\n#### 1. **Device-Based Architecture**\r\n\r\nThe most fundamental pattern is the device-based architecture, where computation units are defined as devices that respond to specific message paths. This architecture enables:\r\n\r\n- **Extensibility**: New functionality can be added by implementing new devices\r\n- **Composition**: Devices can be combined into pipelines using device stacks\r\n- **Polymorphism**: Different implementations can share the same interface\r\n- **Isolation**: Devices operate independently, promoting code separation\r\n\r\n#### 2. **Device Swapping Pattern**\r\n\r\nA key innovation is the device swapping pattern, where devices temporarily replace themselves with other devices to handle specific operations:\r\n\r\n- `dev_process.erl` uses this to route operations to specialized sub-devices\r\n- `dev_stack.erl` uses this to execute each device in a stack sequentially\r\n- `dev_scheduler.erl` uses this to handle different scheduling operations\r\n\r\nThis pattern enables a form of dynamic dispatch while maintaining the cryptographic integrity of the message's HashPath.\r\n\r\n#### 3. **Registry and Factory Patterns**\r\n\r\n`dev_scheduler_registry` implements these patterns:\r\n\r\n- **Registry**: Mapping between process IDs and their Erlang processes\r\n- **Factory**: Creating new server processes when needed\r\n\r\n#### 4. **Process-per-Entity Model**\r\n\r\nMultiple components use dedicated Erlang processes for each HyperBEAM entity:\r\n\r\n- `dev_scheduler_server`: One process per scheduled process ID\r\n- `dev_process_worker`: One process per active process computation\r\n\r\nThis provides isolation and aligns with Erlang's \"let it crash\" philosophy.\r\n\r\n#### 5. **Content-Addressed Storage**\r\n\r\nBoth the scheduler and process components use content-addressed storage:\r\n\r\n- Assignments and computation results are stored by their cryptographic identity\r\n- Symbolic links create navigable hierarchies over the content-addressed store\r\n\r\n#### 6. **Hash Chain Verification**\r\n\r\nCryptographic verification is used throughout:\r\n\r\n- Scheduler assignments are linked through hash chains\r\n- Process messages maintain HashPaths for verification\r\n- Device operations preserve cryptographic verification chains\r\n\r\n#### 7. **Adapters and Formatters**\r\n\r\nFormat conversion patterns appear in multiple places:\r\n\r\n- `dev_scheduler_formats`: Converts between internal and client formats\r\n- `dev_message`: Provides case-insensitive key access and field manipulation\r\n- Device interfaces generally adapt between different representation formats\r\n\r\n#### 8. **Long-Lived Workers with Persistent State**\r\n\r\nFor performance optimization:\r\n\r\n- `dev_process_worker`: Maintains process state in memory between steps\r\n- `dev_scheduler_server`: Maintains scheduler state across interactions\r\n\r\n#### 9. **Bottleneck by Design**\r\n\r\nSome components are deliberately designed as sequential bottlenecks:\r\n\r\n- `dev_scheduler_server`: Ensures scheduler ordering guarantees\r\n- Device stacks in fold mode: Process sequentially to maintain state\r\n\r\n### Component Interactions\r\n\r\nThe components interact in several well-defined workflows across the functional areas:\r\n\r\n#### Device Infrastructure Workflow\r\n\r\n1. Messages are processed by devices, starting with the base device functionality from `dev_message`.\r\n\r\n2. Complex device compositions use `dev_stack` to orchestrate execution:\r\n   - In fold mode, devices execute sequentially, each receiving the state from the previous\r\n   - In map mode, devices execute independently, with results combined in a single message\r\n\r\n3. The device swapping pattern allows temporary replacement of the current device:\r\n   ```\r\n   /Msg1/AlicesExcitingKey ->\r\n       dev_stack:execute ->\r\n           /Msg1/Set?device=/Device-Stack/1 ->\r\n           /Msg2/AlicesExcitingKey ->\r\n           /Msg3/Set?device=/Device-Stack/2 ->\r\n           /Msg4/AlicesExcitingKey\r\n           ... ->\r\n           /MsgN/Set?device=[This-Device] ->\r\n       returns {ok, /MsgN+1} ->\r\n   /MsgN+1\r\n   ```\r\n\r\n#### Process Management Workflow\r\n\r\n1. External clients interact with `dev_process`, which routes operations to the appropriate specialized devices.\r\n\r\n2. For computation operations:\r\n   - The process resolves the appropriate device based on the process type\r\n   - Execution results are stored in `dev_process_cache` with both slot and message ID indexes\r\n   - Long-running computations may be handled by `dev_process_worker` for state preservation\r\n\r\n3. Processes interact with the scheduler to determine execution order:\r\n   - `dev_process` requests slot assignments from `dev_scheduler`\r\n   - The scheduled slots determine the order of execution\r\n\r\n#### Scheduler System Workflow\r\n\r\n1. External clients interact with `dev_scheduler`, which acts as the façade for the scheduler.\r\n\r\n2. When a process needs to be scheduled, `dev_scheduler_registry` is consulted to find the appropriate server or create a new one.\r\n\r\n3. The scheduling request is delegated to the `dev_scheduler_server` instance for the specific process ID.\r\n\r\n4. The server generates an assignment, updates its state, and uses `dev_scheduler_cache` to persist the assignment.\r\n\r\n5. When responses are returned to clients, `dev_scheduler_formats` converts the internal representations to the appropriate client format.\r\n\r\n![Device and Process Management Subsystem Interactions](../diagrams/device_process_subsystem_interactions.png)\r\n\r\n*Note: This diagram is a conceptual representation; the actual image file may need to be generated separately.*\r\n\r\n## Key Mechanisms\r\n\r\nThe Device and Process Management Subsystem implements several key mechanisms that span across its components:\r\n\r\n### Device Resolution and Dispatch\r\n\r\nA core mechanism is the device resolution and dispatch system:\r\n\r\n1. Devices are identified by string names (e.g., `\"process@1.0\"`, `\"stack@1.0\"`)\r\n2. Device resolution maps these names to Erlang modules\r\n3. Devices can be dynamically loaded and unloaded\r\n4. Messages are dispatched to devices based on their path fields\r\n\r\nThis mechanism enables the extensible computation model that powers HyperBEAM.\r\n\r\n### Attestation and Verification\r\n\r\nThe subsystem provides robust security through attestation and verification:\r\n\r\n1. Messages can be cryptographically attested (signed) by devices\r\n2. Attestations can be verified to ensure message integrity\r\n3. Multiple attestors can sign the same message for multi-party verification\r\n4. The attestation chain forms a cryptographic history of message transformations\r\n\r\n### Slot-Based Execution\r\n\r\nBoth process and scheduler components use a slot-based execution model:\r\n\r\n1. Processes track their computation state in numbered slots\r\n2. Schedulers assign work to numbered slots in their execution sequence\r\n3. Slots form a sequential, verifiable history of state transitions\r\n\r\n### Hash Chain Verification\r\n\r\nTo ensure the integrity and sequentiality of operations, the subsystem maintains cryptographic hash chains:\r\n\r\n1. Each scheduler assignment includes a hash chain field linking to previous assignments\r\n2. Messages maintain HashPaths for cryptographic verification of their history\r\n3. Device operations preserve these verification chains\r\n\r\n### Device Composition\r\n\r\nThe device stack mechanism enables sophisticated device composition:\r\n\r\n1. Devices can be arranged in stacks for sequential processing (fold mode)\r\n2. Devices can be executed in parallel with results combined (map mode)\r\n3. Special status returns (`skip`, `pass`) provide flow control within stacks\r\n4. Input/output prefixing provides namespace isolation between devices\r\n\r\n### Scheduling Modes\r\n\r\nThe scheduler supports different modes offering various trade-offs:\r\n\r\n1. **Aggressive**: Responds immediately and performs operations asynchronously\r\n2. **Local Confirmation**: Responds after successfully writing to local storage\r\n3. **Remote Confirmation**: Responds after successfully uploading to the network\r\n\r\n### Persistence with In-Memory Optimization\r\n\r\nThe subsystem optimizes performance while ensuring persistence:\r\n\r\n1. Long-lived workers maintain state in memory for efficiency\r\n2. State is periodically persisted to storage via the cache\r\n3. Workers can reload state from the cache after restarts\r\n4. Content-addressed storage prevents duplication\r\n\r\n### Dual-Indexing for Efficient Retrieval\r\n\r\nProcess computation results are indexed in two ways:\r\n\r\n1. By slot number for sequential access\r\n2. By message ID for content-addressed lookup\r\n\r\nThis dual approach enables efficient state retrieval through multiple paths.\r\n\r\n## Integration with Other Subsystems\r\n\r\nThe Scheduler Subsystem integrates with several other parts of HyperBEAM:\r\n\r\n### Core Infrastructure\r\n\r\n- Uses `hb_converge` for message field access and resolution.\r\n- Uses `hb_message` for message attestation and verification.\r\n- Uses `hb_opts` for configuration and defaults.\r\n- Uses `hb_name` for process registration and lookup.\r\n\r\n### Storage Subsystem\r\n\r\n- Relies on `hb_store` for storage operations and symbolic link management.\r\n- Uses `hb_cache` for content-addressed storage of assignments.\r\n\r\n### Network Communication Subsystem\r\n\r\n- Uses `hb_client` for uploading assignments to the network.\r\n- Potentially waits for network confirmation in `remote_confirmation` mode.\r\n\r\n### Arweave Integration Subsystem\r\n\r\n- Uses `ar_timestamp` to obtain blockchain timing information.\r\n- Incorporates blockchain metadata into assignments.\r\n\r\n## Security Considerations\r\n\r\nThe Device and Process Management Subsystem incorporates several security features:\r\n\r\n1. **Cryptographic Verification**: Hash chains and HashPaths ensure the integrity and sequentiality of operations.\r\n\r\n2. **Message Attestation**: Messages can be cryptographically attested by devices, creating verifiable signatures.\r\n\r\n3. **Multi-Party Verification**: Multiple attestors can sign the same message, enabling multi-party trust.\r\n\r\n4. **Content-Addressed Storage**: Content is identified by its cryptographic hash, preventing substitution attacks.\r\n\r\n5. **Private Field Protection**: `dev_message` prevents access to private fields through its API, maintaining information hiding.\r\n\r\n6. **Single-Writer Design**: The process-per-entity model ensures that only one process can write to a given slot.\r\n\r\n7. **Device Isolation**: Devices operate independently, reducing the impact of compromised devices.\r\n\r\n## Performance Characteristics\r\n\r\nThe performance of the Scheduler Subsystem is influenced by several factors:\r\n\r\n1. **Scheduling Mode**: The choice of scheduling mode significantly impacts latency:\r\n   - Aggressive mode provides the lowest latency but fewer guarantees.\r\n   - Remote confirmation mode provides the strongest guarantees but highest latency.\r\n\r\n2. **Storage Backend**: The choice of storage backend (`hb_store` implementation) affects persistence performance.\r\n\r\n3. **Process Isolation**: The process-per-entity model provides isolation but has overhead for large numbers of processes.\r\n\r\n4. **Symbolic Link Hierarchy**: The use of symbolic links in the cache provides efficient lookup but can have overhead for creation.\r\n\r\n## Future Considerations\r\n\r\nBased on the analysis, several areas warrant consideration for future development:\r\n\r\n1. **Wallet Isolation**: The current TODO comment about potentially using different wallets per scheduler unit suggests a security or isolation consideration that might be addressed in future versions.\r\n\r\n2. **Cache Eviction Strategy**: There's no explicit mention of a cache eviction strategy for older assignments. As processes accumulate many assignments over time, a pruning mechanism might be necessary.\r\n\r\n3. **Distributed Consensus**: In a distributed environment, how are conflicts between multiple nodes scheduling the same process resolved? This may require additional coordination mechanisms.\r\n\r\n4. **Format Evolution**: With the JSON format described as \"legacy,\" there may be plans for new formats in the future.\r\n\r\n## Conclusion\r\n\r\nThe Device and Process Management Subsystem forms the computational heart of HyperBEAM, providing a flexible, secure, and performant framework for distributed computation. Its innovative device-based architecture, combined with sophisticated state management and cryptographic verification, creates a system that is both extensible and robust.\r\n\r\nThe subsystem successfully balances several key concerns:\r\n\r\n1. **Extensibility**: Through the device-based architecture and composition patterns\r\n2. **Security**: Via cryptographic verification, attestation, and content addressing\r\n3. **Performance**: With in-memory workers, caching, and optimization strategies\r\n4. **Reliability**: Through persistence, recovery mechanisms, and confirmation modes\r\n5. **Interoperability**: Through format conversion and standardized interfaces\r\n\r\nThe device-based approach, with its modular design and clear separation of concerns, enables HyperBEAM to grow and adapt to new requirements while maintaining its core guarantees. The slot-based execution model, coupled with cryptographic verification chains, ensures that all operations are auditable and tamper-evident.\r\n\r\nThis subsystem exemplifies HyperBEAM's architectural philosophy of combining Erlang's process model with content-addressed storage and cryptographic verification to create reliable, extensible distributed systems. By understanding how these components work together, developers can leverage the full power of HyperBEAM's computation model, extending it with new devices and integrating it with external systems while maintaining its security and integrity guarantees.\r\n"}}